ggplot(f_score_results_by_available_recordings_for_selected_species, aes(x=log2(total_online_recordings), y=f025_score, label=label)) +
geom_point(color = "black") +
geom_text_repel(size = 2.5,
nudge_y = 0.02) +
geom_smooth(method = lm, se = FALSE) +
xlab("Number of recordings, log2 scale") +
ylab("Mean F0.25 score") +
ggtitle("F0.25 score by the number of foreground recordings on XC and ML for each species\n(only species with ≥30 seconds of total vocalization time and ≥10 detections)") +
theme_bw()
dev.off()
selected_recall_results_by_available_recordings_for_selected_species <- BirdNET_PR_results_by_species %>%
filter(confidence_threshold == 0.1) %>%
# mutate(species = gsub("Dendrocoptes medius", "Leiopicus medius", species)) %>%
right_join(recall_results_by_available_recordings_for_selected_species %>%
select(label, species, total_online_recordings)) %>%
# mutate(label = gsub("Den med", "Lei med", label)) %>%
group_by(label, total_online_recordings) %>%
summarize(rec_recall = mean(recall))
jpeg(file = paste0(R_plots_path, "/", algorithm, " recall analysis/recall_by_online_recordings_(only_species_with_at_least_30_seconds).jpeg"), width = 4000, height = 3000, res = 500)
ggplot(selected_recall_results_by_available_recordings_for_selected_species, aes(x=log2(total_online_recordings), y=rec_recall, label=label)) +
geom_point(color = "black") +
geom_text_repel(size = 2.5,
nudge_y = 0.02) +
geom_smooth(method = lm, se = FALSE) +
xlab("Number of recordings, log2 scale") +
ylab("Mean rec_recall") +
ggtitle("Recall score by the number of foreground recordings on XC and ML for each species\n(only species with ≥30 seconds of total vocalization time)") +
theme_bw()
dev.off()
selected_precision_results_by_available_recordings_for_selected_species <- BirdNET_PR_results_by_species %>%
filter(confidence_threshold == 0.1) %>%
# mutate(species = gsub("Dendrocoptes medius", "Leiopicus medius", species)) %>%
left_join(online_foreground_recordings_by_species %>%
select(species, total_online_recordings)) %>%
left_join(species_labels,
by = "species") %>%
filter(species %in% precision_results_by_available_recordings_for_selected_species$species,
!is.na(species)) %>%
group_by(label, total_online_recordings) %>%
# mutate(label = gsub("Den med", "Lei med", label)) %>%
summarize(rec_precision = mean(precision))
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision analysis/precision_by_online_recordings_(only_species_with_at_least_10_detections).jpeg"), width = 4000, height = 3000, res = 500)
ggplot(selected_precision_results_by_available_recordings_for_selected_species, aes(x=log2(total_online_recordings), y=rec_precision, label=label)) +
geom_point(color = "black") +
geom_text_repel(size = 2.5,
nudge_y = 0.02) +
geom_smooth(method = lm, se = FALSE) +
xlab("Number of recordings, log2 scale") +
ylab("Mean rec_precision") +
ggtitle("Precision score by the number of foreground recordings on XC and ML for each species\n(only species with ≥10 detections)") +
theme_bw()
dev.off()
# Checking the normality of the F1-score distribution to determine whether we can perform a Pearson correlation analysis or not
shapiro.test(f_score_results_by_available_recordings_for_selected_species$f1_score)
hist(f_score_results_by_available_recordings_for_selected_species$f1_score)
shapiro.test(log2(f_score_results_by_available_recordings_for_selected_species$total_online_recordings))
hist(log2(f_score_results_by_available_recordings_for_selected_species$total_online_recordings))
# The f1-score distribution is not normal. Therefore, we can't perform Pearson's correlation analysis.
cor.test(log2(f_score_results_by_available_recordings_for_selected_species$total_online_recordings), f_score_results_by_available_recordings_for_selected_species$f1_score, method = "spearman")
# Spearman's correlation coefficient is -0.58, with a p-value of 3.4e-4, which indicates that the total number of foreground recordings on Xeno-canto and the Macaulay Library does have a significant negative influence on F1 score results.
cor.test(log2(f_score_results_by_available_recordings_for_selected_species$total_online_recordings), f_score_results_by_available_recordings_for_selected_species$f025_score, method = "spearman")
# Spearman's correlation coefficient is -0.28, with a p-value of 0.108, which indicates that the total number of foreground recordings on Xeno-canto and the Macaulay Library doesn't have a significant influence on F0.25 score results.
cor.test(log2(selected_precision_results_by_available_recordings_for_selected_species$total_online_recordings), selected_precision_results_by_available_recordings_for_selected_species$rec_precision, method = "spearman")
# Spearman's correlation coefficient is 0.438, with a p-value of 4.66e-4, which indicates that the total number of foreground recordings on Xeno-canto and the Macaulay Library has a significant influence on precision results.
cor.test(log2(selected_recall_results_by_available_recordings_for_selected_species$total_online_recordings), selected_recall_results_by_available_recordings_for_selected_species$rec_recall, method = "spearman")
# Spearman's correlation coefficient is -0.489, with a p-value of 1.17e-3, which indicates that the total number of foreground recordings on Xeno-canto and the Macaulay Library has a significant influence on recall results.
#--------------Global detection results by confidence threshold-----------------
annotations <- read.csv("../CSVs/annotations.csv")
recordings <- read.csv("../CSVs/recordings_extended.csv")
for (i in 1:length(confidence_levels)) {
temp_global_detection_results <- BirdNET_precision_results %>%
filter(confidence >= confidence_levels[i]) %>%
group_by(recording_id, species) %>%
summarize(correct_identifications = sum(as.numeric(correct_identification)),
incorrect_identifications = sum(as.numeric(!correct_identification))) %>%
mutate(correct_identifications = case_when(correct_identifications > 0 ~ 1,
TRUE ~ 0),
incorrect_identifications = case_when(incorrect_identifications > 0 ~ 1,
TRUE ~ 0)) %>%
group_by(species) %>%
summarize(recordings_where_correctly_identified = sum(correct_identifications),
recordings_where_incorrectly_identified = sum(incorrect_identifications))
temp_detection_results_by_habitat <- BirdNET_precision_results %>%
left_join(recordings %>%
select(recording_id, habitat),
by = "recording_id") %>%
filter(confidence >= confidence_levels[i]) %>%
group_by(habitat, species) %>%
summarize(correct_identifications = sum(as.numeric(correct_identification)),
incorrect_identifications = sum(as.numeric(!correct_identification))) %>%
mutate(correct_identifications = case_when(correct_identifications > 0 ~ 1,
TRUE ~ 0),
incorrect_identifications = case_when(incorrect_identifications > 0 ~ 1,
TRUE ~ 0)) %>%
group_by(habitat) %>%
summarize(correctly_identified_species = sum(correct_identifications),
incorrectly_identified_species = sum(incorrect_identifications))
if (i == 1) {
global_BirdNET_detection_results_1st_threshold <- temp_global_detection_results
BirdNET_detection_results_by_habitat_1st_threshold <- temp_detection_results_by_habitat
} else if (i == 2) {
global_BirdNET_detection_results_2nd_threshold <- temp_global_detection_results
BirdNET_detection_results_by_habitat_2nd_threshold <- temp_detection_results_by_habitat
} else {
global_BirdNET_detection_results_3rd_threshold <- temp_global_detection_results
BirdNET_detection_results_by_habitat_3rd_threshold <- temp_detection_results_by_habitat
}
}
global_detection_results <- total_birds_detected %>%
filter(species != "Unidentified bird") %>%
select(species, n_recordings_clearly_detected) %>%
full_join(global_BirdNET_detection_results_1st_threshold,
by = "species") %>%
rename("correctly_identified_1st_threshold" = "recordings_where_correctly_identified",
"incorrectly_identified_1st_threshold" = "recordings_where_incorrectly_identified") %>%
full_join(global_BirdNET_detection_results_2nd_threshold,
by = "species") %>%
rename("correctly_identified_2nd_threshold" = "recordings_where_correctly_identified",
"incorrectly_identified_2nd_threshold" = "recordings_where_incorrectly_identified") %>%
full_join(global_BirdNET_detection_results_3rd_threshold,
by = "species") %>%
rename("correctly_identified_3rd_threshold" = "recordings_where_correctly_identified",
"incorrectly_identified_3rd_threshold" = "recordings_where_incorrectly_identified") %>%
arrange(species) %>%
ungroup()
global_detection_results[is.na(global_detection_results)] <- 0
global_detection_results <- global_detection_results %>%
mutate(non_present_species_1st_threshold = as.numeric(n_recordings_clearly_detected == 0 & incorrectly_identified_1st_threshold > 0) * incorrectly_identified_1st_threshold,
non_present_species_2nd_threshold = as.numeric(n_recordings_clearly_detected == 0 & incorrectly_identified_2nd_threshold > 0) * incorrectly_identified_2nd_threshold,
non_present_species_3rd_threshold = as.numeric(n_recordings_clearly_detected == 0 & incorrectly_identified_3rd_threshold > 0) * incorrectly_identified_3rd_threshold)
global_detection_results <- rbind(global_detection_results, c("Total detections",
sum(global_detection_results$n_recordings_clearly_detected),
sum(global_detection_results$correctly_identified_1st_threshold),
sum(global_detection_results$incorrectly_identified_1st_threshold),
sum(global_detection_results$correctly_identified_2nd_threshold),
sum(global_detection_results$incorrectly_identified_2nd_threshold),
sum(global_detection_results$correctly_identified_3rd_threshold),
sum(global_detection_results$incorrectly_identified_3rd_threshold),
sum(global_detection_results$non_present_species_1st_threshold),
sum(global_detection_results$non_present_species_2nd_threshold),
sum(global_detection_results$non_present_species_3rd_threshold)))
global_detection_results <- rbind(global_detection_results, c("Total species",
sum(as.numeric(global_detection_results$n_recordings_clearly_detected > 0)) - 1,
sum(as.numeric(global_detection_results$correctly_identified_1st_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$incorrectly_identified_1st_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$correctly_identified_2nd_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$incorrectly_identified_2nd_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$correctly_identified_3rd_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$incorrectly_identified_3rd_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$non_present_species_1st_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$non_present_species_2nd_threshold > 0)) - 1,
sum(as.numeric(global_detection_results$non_present_species_3rd_threshold > 0)) - 1))
global_detection_results_file_name <- file.path(paste0(CSV_path), "global_detection_results.csv")
write.csv(x=global_detection_results, file=global_detection_results_file_name, row.names = FALSE)
#-------------Global detection results by time of day and habitat---------------
bird_species_richness_by_recording_and_time <- annotations %>%
left_join(recordings, by = "recording_id") %>%
filter(confidence_level == 1, sound_category == "Bird") %>%
select(recording_id, recording_date, recording_time, label) %>%
unique() %>%
group_by(recording_id, recording_date, recording_time) %>%
summarize(n_bird_species = n())
bird_species_richness_by_time <- bird_species_richness_by_recording_and_time %>%
group_by(recording_time) %>%
summarize(mean_richness = mean(n_bird_species))
jpeg(file = "../R plots/Annotation analysis/number_of_species_detected_by_time_of_the_day.jpeg", width = 3000, height = 2400, res = 500)
ggplot(bird_species_richness_by_recording_and_time, aes(x = as.factor(recording_time), y = n_bird_species, fill = as.factor(recording_time))) +
geom_boxplot() +
xlab("Time of the day") +
ylab("Bird species detected") +
ggtitle("Bird species detected per minute by time of the day") +
theme_bw() +
theme(legend.position = "none") +
scale_fill_manual(values = c("#84B8E1", "#84B8E1", "#84B8E1"))
dev.off()
# Checking the normality of the distribution of the number of species detected per recording to determine whether we can perform an ANOVA analysis or not
shapiro.test(bird_species_richness_by_recording_and_time$n_bird_species)
hist(bird_species_richness_by_recording_and_time$n_bird_species)
# The distribution of the number of species per recording is not normal. Therefore, we have to transform the data before we can perform the ANOVA analysis.
# Nonetheless, neither sqrt nor logarithmic transformations transform the distribution into a normal one.
# Thus, a Kruskal-Wallis test will have to be performed insted of an ANOVA.
kruskal.test(n_bird_species ~ recording_time, data = bird_species_richness_by_recording_and_time)
# Performing t-tests to check whether the differences between the number of species detected at 7am and at 1pm are statistically significant.
t.test((bird_species_richness_by_recording_and_time %>%
filter(recording_time == "07:00:00"))$n_bird_species,
(bird_species_richness_by_recording_and_time %>%
filter(recording_time == "13:00:00"))$n_bird_species)
# Same test, this time comparing the number of species detected at 7am and at 0am.
t.test((bird_species_richness_by_recording_and_time %>%
filter(recording_time == "07:00:00"))$n_bird_species,
(bird_species_richness_by_recording_and_time %>%
filter(recording_time == "00:00:00"))$n_bird_species)
bird_species_richness_by_recording_and_habitat <- annotations %>%
left_join(recordings, by = "recording_id") %>%
filter(confidence_level == 1, sound_category == "Bird") %>%
select(station_id, habitat, label) %>%
unique() %>%
group_by(station_id, habitat) %>%
summarize(n_bird_species = n())
jpeg(file = "../R plots/Annotation analysis/number_of_species_detected_by_habitat.jpeg", width = 3000, height = 2400, res = 500)
ggplot(bird_species_richness_by_recording_and_habitat, aes(x = as.factor(habitat), y = n_bird_species, fill = as.factor(habitat))) +
geom_boxplot() +
xlab("Habitat") +
ylab("Bird species detected") +
ggtitle("Bird species detected per day by habitat") +
theme_bw() +
theme(legend.position = "none") +
scale_fill_manual(values = c("#84B8E1", "#84B8E1", "#84B8E1", "#84B8E1", "#84B8E1"))
dev.off()
# Checking the normality of the distribution of the number of species detected per recording to determine whether we can perform an ANOVA analysis or not
shapiro.test(bird_species_richness_by_recording_and_habitat$n_bird_species)
hist(bird_species_richness_by_recording_and_habitat$n_bird_species)
# The distribution of the number of species per recording can't be proven not to be normal. Therefore, we don't have to transform the data before we can perform the ANOVA analysis.
anova_species_richness_by_habitat <- aov(n_bird_species ~ as.factor(habitat), data = bird_species_richness_by_recording_and_habitat)
summary(anova_species_richness_by_habitat)
n_recordings_by_habitat <- recordings %>%
count(habitat) %>%
rename("Recordings analyzed" = "n")
bird_species_richness_by_habitat <- annotations %>%
left_join(recordings, by = "recording_id") %>%
filter(confidence_level == 1, sound_category == "Bird") %>%
select(recording_id, recording_date, habitat, label) %>%
unique() %>%
group_by(habitat, label) %>%
summarize(n_detections = n()) %>%
count(habitat) %>%
rename("Species annotated" = "n")
detection_results_by_habitat <- n_recordings_by_habitat %>%
left_join(bird_species_richness_by_habitat,
by = "habitat") %>%
left_join(BirdNET_detection_results_by_habitat_1st_threshold,
by = "habitat") %>%
rename("Correctly_identified_1st_threshold" = "correctly_identified_species",
"Incorrectly_identified_1st_threshold" = "incorrectly_identified_species") %>%
left_join(BirdNET_detection_results_by_habitat_2nd_threshold,
by = "habitat") %>%
rename("Correctly_identified_2nd_threshold" = "correctly_identified_species",
"Incorrectly_identified_2nd_threshold" = "incorrectly_identified_species") %>%
left_join(BirdNET_detection_results_by_habitat_3rd_threshold,
by = "habitat") %>%
rename("Correctly_identified_3rd_threshold" = "correctly_identified_species",
"Incorrectly_identified_3rd_threshold" = "incorrectly_identified_species",
"Habitat" = "habitat")
detection_results_by_habitat_file_name <- file.path(paste0("../CSVs", algorithm_text, BirdNET_version, ds_text, filtered_text), "detection_results_by_habitat.csv")
write.csv(x=detection_results_by_habitat, file=detection_results_by_habitat_file_name, row.names = FALSE)
# It calculates the Precision-Recall AUC for each habitat
BirdNET_PR_results_by_recording <- read.csv(paste0(CSV_path, "/", algorithm, "_PR_results_by_recording.csv"))
recording_info_extended <- read.csv("../CSVs/recordings_extended.csv")
BirdNET_PR_results_by_recording[is.na(BirdNET_PR_results_by_recording)] = 0
BirdNET_PR_results_by_recording_and_habitat <- BirdNET_PR_results_by_recording %>%
mutate(confidence_threshold = as.numeric(confidence_threshold)) %>%
left_join(recording_info_extended %>%
select(recording_id, habitat),
by = "recording_id") %>%
left_join(recording_acoustic_data %>%
select(recording_id, NDSI_left),
by = "recording_id")
pr_auc_by_recording <- data.frame()
for (i in 1:nrow(recordings)) {
BirdNET_PR_results_by_selected_recording <- BirdNET_PR_results_by_recording_and_habitat %>%
filter(recording_id == recordings[i,]$recording_id)
pr_auc <- abs(trapz(BirdNET_PR_results_by_selected_recording$precision, BirdNET_PR_results_by_selected_recording$recall))
pr_auc_by_recording <- rbind(pr_auc_by_recording, c(recordings[i,]$recording_id, pr_auc))
}
colnames(pr_auc_by_recording) <- c("recording_id", "AUC")
BirdNET_PR_results_by_recording_and_habitat <- BirdNET_PR_results_by_recording_and_habitat %>%
left_join(pr_auc_by_recording,
by = "recording_id") %>%
mutate(habitat = as.factor(habitat)) %>%
select(recording_id, habitat, NDSI_left, recall, precision, confidence_threshold) %>%
rename("rec_recall" = "recall",
"rec_precision" = "precision")
BirdNET_PR_results_by_recording_and_habitat_conf10 <- BirdNET_PR_results_by_recording_and_habitat %>%
filter(confidence_threshold == 0.1) %>%
select(-confidence_threshold)
BirdNET_PR_results_by_recording_and_habitat_conf35 <- BirdNET_PR_results_by_recording_and_habitat %>%
filter(confidence_threshold == 0.35) %>%
select(-confidence_threshold)
# Checking the normality of the distribution of the recall and precision variables to determine whether we can perform an ANOVA analysis or not
shapiro.test(BirdNET_PR_results_by_recording_and_habitat_conf10$rec_recall)
hist(BirdNET_PR_results_by_recording_and_habitat_conf10$rec_recall)
shapiro.test(BirdNET_PR_results_by_recording_and_habitat_conf10$rec_precision)
hist(BirdNET_PR_results_by_recording_and_habitat_conf10$rec_precision)
# The distribution of recall and precision values is not normal. Therefore, we have to transform them before we can perform the ANOVA analysis.
# Nonetheless, neither sqrt nor logarithmic transformations transform the recall and precision distributions into normal ones.
# Thus, a Kruskal-Wallis test will have to be performed insted of an ANOVA.
kruskal.test(rec_recall ~ habitat, data = BirdNET_PR_results_by_recording_and_habitat_conf10)
kruskal.test(rec_precision ~ habitat, data = BirdNET_PR_results_by_recording_and_habitat_conf10)
BirdNET_PR_results_by_recording_and_habitat_conf10 <- BirdNET_PR_results_by_recording_and_habitat_conf10 %>%
mutate(rec_F1_score = 2 * 1 * rec_precision * rec_recall / (rec_precision + rec_recall))
BirdNET_PR_results_by_recording_and_habitat_conf10[is.na(BirdNET_PR_results_by_recording_and_habitat_conf10)] = 0
kruskal.test(rec_F1_score ~ habitat, data = BirdNET_PR_results_by_recording_and_habitat_conf10)
# Habitat does not seem to have a significant impact on neither recall nor precision.
BirdNET_recall_results_by_recording_conf10 <- melt(setDT(BirdNET_PR_results_by_recording_and_habitat_conf10 %>%
select(recording_id, rec_recall)), id.vars = "recording_id", variable.name = "metric", value.name = "value")
BirdNET_precision_results_by_recording_conf10 <- melt(setDT(BirdNET_PR_results_by_recording_and_habitat_conf10 %>%
select(recording_id, rec_precision)), id.vars = "recording_id", variable.name = "metric", value.name = "value")
BirdNET_recall_results_by_recording_conf35 <- melt(setDT(BirdNET_PR_results_by_recording_and_habitat_conf35 %>%
select(recording_id, rec_recall)), id.vars = "recording_id", variable.name = "metric", value.name = "value")
BirdNET_precision_results_by_recording_conf35 <- melt(setDT(BirdNET_PR_results_by_recording_and_habitat_conf35 %>%
select(recording_id, rec_precision)), id.vars = "recording_id", variable.name = "metric", value.name = "value")
BirdNET_recall_and_precision_results_by_habitat_conf35 <- union_all(BirdNET_recall_results_by_recording_conf35, BirdNET_precision_results_by_recording_conf35) %>%
left_join(BirdNET_PR_results_by_recording_and_habitat_conf35 %>%
select(recording_id, habitat),
by = "recording_id")
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/precision_and_recall_by_habitat.jpeg"), width = 3000, height = 2400, res = 500)
ggplot(BirdNET_recall_and_precision_results_by_habitat_conf35, aes(x = habitat, y = value, fill = metric)) +
geom_boxplot() +
xlab("Habitat") +
ylab("Value") +
labs(fill = "Metric") +
ggtitle("rec_precision and rec_recall distributions by dominant habitat") +
theme_bw() +
scale_fill_manual(values = c("#E4959C", "#95B5DA"))
dev.off()
# Checking the normality of the NDSI index distribution to determine whether we can perform a Pearson correlation analysis or not
shapiro.test(BirdNET_PR_results_by_recording_and_habitat_conf10$NDSI_left)
hist(BirdNET_PR_results_by_recording_and_habitat_conf10$NDSI_left)
# The distribution of NDSI, recall and precision values is not normal.
# Therefore, we have to transform them before we can perform the Pearson's correlation analysis.
# Nonetheless, neither sqrt nor logarithmic transformations transform the NDSI, recall or precision distributions into normal ones.
# Thus, a Spearman's correlation test will have to be performed insted of a Pearson's one.
# The NDSI index has a significant influence on the precision of the algorithm (r = 0.128, p-value = 0.053)
cor.test(BirdNET_PR_results_by_recording_and_habitat_conf10$NDSI_left, BirdNET_PR_results_by_recording_and_habitat_conf10$rec_precision, method = "spearman")
# The NDSI index has a significant influence on the recall of the algorithm (r = 0.283, p-value = 1.463e-5)
cor.test(BirdNET_PR_results_by_recording_and_habitat_conf10$NDSI_left, BirdNET_PR_results_by_recording_and_habitat_conf10$rec_recall, method = "spearman")
BirdNET_F1_results_by_recording_and_habitat_conf10 <- BirdNET_PR_results_by_recording_and_habitat_conf10 %>%
mutate(rec_F1_score = 2 * 1 * rec_precision * rec_recall / (rec_precision + rec_recall))
BirdNET_F1_results_by_recording_and_habitat_conf10[is.na(BirdNET_F1_results_by_recording_and_habitat_conf10)] = 0
# The NDSI index has a significant influence on the F1-score of the algorithm (r = 0.144, p-value = 0.03)
cor.test(BirdNET_F1_results_by_recording_and_habitat_conf10$NDSI_left, BirdNET_F1_results_by_recording_and_habitat_conf10$rec_F1, method = "spearman")
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision analysis/precision_by_anthropic_noise.jpeg"), width = 3200, height = 2500, res = 500)
ggplot(BirdNET_PR_results_by_recording_and_habitat_conf10, aes(x=NDSI_left, y=rec_precision)) +
geom_point(color = "black") +
geom_smooth(method = lm, se = TRUE) +
xlab("NDSI score") +
ylab("rec_precision") +
ggtitle(paste0(algorithm, " precision by NDSI score")) +
theme(panel.background = element_blank(),
axis.line = element_line())
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " recall analysis/recall_by_anthropic_noise.jpeg"), width = 3200, height = 2500, res = 500)
ggplot(BirdNET_PR_results_by_recording_and_habitat_conf10, aes(x=NDSI_left, y=rec_recall)) +
geom_point(color = "black") +
geom_smooth(method = lm, se = TRUE) +
xlab("NDSI score") +
ylab("rec_recall") +
ggtitle(paste0(algorithm, " recall by NDSI score")) +
theme(panel.background = element_blank(),
axis.line = element_line())
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/F1-score_by_anthropic_noise.jpeg"), width = 3200, height = 2500, res = 500)
ggplot(BirdNET_F1_results_by_recording_and_habitat_conf10, aes(x=NDSI_left, y=rec_F1_score)) +
geom_point(color = "black") +
geom_smooth(method = lm, se = TRUE) +
xlab("NDSI score") +
ylab("F1-score at the recording level") +
ggtitle(paste0(algorithm, " F1-score by NDSI score")) +
theme(panel.background = element_blank(),
axis.line = element_line())
dev.off()
# ------------------------Recall by sample size analysis------------------------
global_detection_results <- read.csv(paste0(CSV_path, "/global_detection_results_by_recording_subset.csv"))
grouped_global_detection_results <- global_detection_results %>%
group_by(n_recordings_sampled, minimum_confidence_threshold) %>%
summarize(ds_TPR = mean(TPR),
ds_FPR = mean(FPR),
ds_F1 = mean(F1),
ds_TPR_to_FPR_ratio = ds_TPR/ds_FPR,
ds_TPR_minus_FPR = mean(TPR-FPR),
proportion_of_correct_species = mean(n_species_correctly_detected / (n_species_correctly_detected + n_species_incorrectly_detected)))
jpeg(file = paste0(R_plots_path, "/", algorithm, " recall analysis/ds_TPR_by_n_recordings_sampled.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_TPR, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("ds_TPR") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("True Positive Rate at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " recall analysis/ds_FPR_by_n_recordings_sampled.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_FPR, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("ds_FPR") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("False Positive Rate at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " recall analysis/ds_F1-score_by_n_recordings_sampled.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_stacked_bar_plot.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = dodge, stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = "dodge", stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_non-stacked_bar_plot.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = "dodge", stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_unstacked_bar_plot.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = "dodge", stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_unstacked_bar_plot.jpeg"), width = 5000, height = 3200, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = "dodge", stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("F1-score at the dataset level") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_unstacked_bar_plot.jpeg"), width = 5000, height = 3200, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = "dodge", stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("ds_F1-score") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
jpeg(file = paste0(R_plots_path, "/", algorithm, " precision and recall analysis/ds_F1-score_by_n_recordings_sampled_stacked_bar_plot.jpeg"), width = 4000, height = 2500, res = 500)
ggplot(grouped_global_detection_results, aes(x = as.numeric(n_recordings_sampled), y = ds_F1, fill = as.factor(minimum_confidence_threshold))) +
geom_bar(position = position_nudge(x = 0.5), stat = "identity",
colour = 'black') +
xlab("Number of recordings sampled") +
ylab("ds_F1-score") +
ylim(0, 0.75) +
labs(fill = "Minimum confidence\nthreshold used") +
scale_fill_manual(values = c("#2b4056", "#3874a5", "#56b1f7")) +
ggtitle("F1-score at the dataset level by number of recordings sampled") +
theme_bw()
dev.off()
source("~/Escriptori/Màster Ecologia/TFM/R scripts/compile_PR_results.R", echo=TRUE)
